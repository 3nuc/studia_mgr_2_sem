{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DET\nNOUN\nAUX\nDET\nNOUN\nDET\nVERB\nAUX\nVERB\nPART\nVERB\nADP\nNOUN\nADP\nADJ\nCCONJ\nADJ\nNOUN\nADV\nADP\nNOUN\nNOUN\nPUNCT\nADJ\nNOUN\nAUX\nDET\nNOUN\nPART\nVERB\nADJ\nNOUN\nADP\nNOUN\nPUNCT\nVERB\nNOUN\nPUNCT\nDET\nNOUN\nVERB\nNOUN\nPART\nVERB\nDET\nADV\nADJ\nNOUN\nADP\nNOUN\nPUNCT\nDET\nPUNCT\nADJ\nPUNCT\nNOUN\nVERB\nDET\nNOUN\nPUNCT\nDET\nNOUN\nNOUN\nPUNCT\nADJ\nNOUN\nPUNCT\nPUNCT\nCCONJ\nADJ\nNOUN\nVERB\nCCONJ\nVERB\nADP\nPUNCT\nADJ\nPUNCT\nNOUN\nVERB\nAUX\nVERB\nADP\nSCONJ\nDET\nNOUN\nNOUN\nPUNCT\nDET\nNOUN\nVERB\nADV\nADV\nAUX\nVERB\nADP\nDET\nNOUN\nADP\nNOUN\nDET\nAUX\nVERB\nCCONJ\nVERB\nADV\nPUNCT\nADP\nADJ\nDET\nNOUN\nNOUN\nCCONJ\nNOUN\nNOUN\nPUNCT\nSPACE\nNOUN\nAUX\nVERB\nSCONJ\nNOUN\nNOUN\nADP\nDET\nADJ\nNOUN\nADP\nADJ\nCCONJ\nNOUN\nNOUN\nPUNCT\nDET\nVERB\nADJ\nADJ\nNOUN\nNOUN\nSCONJ\nNOUN\nNOUN\nCCONJ\nADJ\nNOUN\nPUNCT\nNOUN\nNOUN\nADJ\nSCONJ\nADJ\nNOUN\nCCONJ\nNOUN\nPUNCT\nVERB\nNOUN\nPUNCT\nCCONJ\nADV\nADJ\nNOUN\nNOUN\nSCONJ\nADJ\nNOUN\nCCONJ\nADJ\nNOUN\nADJ\nSCONJ\nNOUN\nPUNCT\nDET\nNOUN\nAUX\nVERB\nADP\nNOUN\nCCONJ\nPRON\nVERB\nNOUN\nADP\nNOUN\nADP\nADJ\nNOUN\nCCONJ\nDET\nNOUN\nPUNCT\nSPACE\nADJ\nNOUN\nAUX\nADV\nVERB\nSCONJ\nVERB\nNOUN\nPUNCT\nSCONJ\nADJ\nNOUN\nPUNCT\nADJ\nADJ\nNOUN\nSCONJ\nDET\nNOUN\nVERB\nNOUN\nADP\nVERB\nNOUN\nPUNCT\nADV\nADP\nDET\nPROPN\nPROPN\nPUNCT\nDET\nADJ\nNOUN\nAUX\nVERB\nPART\nVERB\nADJ\nADJ\nNOUN\nPUNCT\nADJ\nSCONJ\nVERB\nNOUN\nADP\nNOUN\nPUNCT\nADV\nADJ\nADJ\nNOUN\nAUX\nADJ\nNOUN\nNOUN\nADP\nDET\nADJ\nADJ\nNOUN\nPUNCT\nDET\nADJ\nADJ\nADJ\nNOUN\nNOUN\nAUX\nVERB\nADP\nPROPN\nPROPN\nPROPN\nPUNCT\nDET\nADJ\nNOUN\nNOUN\nADP\nDET\nADJ\nNOUN\nAUX\nVERB\nADP\nDET\nNOUN\nPUNCT\nVERB\nPROPN\nPUNCT\nPROPN\nNOUN\nPUNCT\nCCONJ\nPROPN\nPROPN\nPROPN\nPUNCT\nPROPN\nPUNCT\nNOUN\nNOUN\nADP\nDET\nADJ\nNOUN\nPUNCT\nVERB\nADP\nDET\nNOUN\nCCONJ\nDET\nPROPN\nNOUN\nADP\nDET\nNOUN\nPUNCT\nDET\nNOUN\nPUNCT\nNOUN\nCCONJ\nNOUN\nADP\nNOUN\nAUX\nAUX\nVERB\nADV\nADV\nSCONJ\nADV\nPUNCT\nADP\nPROPN\nNOUN\nNOUN\nVERB\nADP\nDET\nADJ\nNOUN\nPUNCT\nSCONJ\nVERB\nADP\nPROPN\nPART\nNOUN\nPUNCT\nPUNCT\nVERB\nADP\nDET\nPROPN\nPROPN\nADP\nDET\nADJ\nNOUN\nADP\nADJ\nADJ\nNOUN\nPUNCT\nSPACE\nADV\nPUNCT\nDET\nADJ\nNOUN\nVERB\nADP\nADV\nADV\nNUM\nNOUN\nNOUN\nPUNCT\nADV\nDET\nADJ\nNOUN\nNOUN\nPUNCT\nPROPN\nPUNCT\nADP\nDET\nNOUN\nADP\nDET\nNOUN\nPUNCT\nNOUN\nPUNCT\nNOUN\nPUNCT\nPROPN\nPUNCT\nNOUN\nPUNCT\nADP\nADP\nDET\nNOUN\nADP\nNOUN\nNOUN\nPUNCT\nADV\nPROPN\nNOUN\nNOUN\nNOUN\nPUNCT\nDET\nNOUN\nNOUN\nVERB\nADP\nADJ\nCCONJ\nADJ\nNOUN\nPUNCT\nCCONJ\nDET\nNOUN\nCCONJ\nNOUN\nNOUN\nVERB\nVERB\nDET\nNOUN\nADP\nNOUN\nADP\nNOUN\nADP\nADJ\nNOUN\nPUNCT\nADJ\nNOUN\nVERB\nNOUN\nNOUN\nPUNCT\nNOUN\nPUNCT\nNOUN\nPUNCT\nPROPN\nPUNCT\nX\nPUNCT\nPUNCT\nPUNCT\nNOUN\nNOUN\nPUNCT\nNOUN\nNOUN\nPUNCT\nNOUN\nPUNCT\nX\nPUNCT\nPUNCT\nPUNCT\nCCONJ\nNOUN\nSYM\nNOUN\nNOUN\nDET\nVERB\nDET\nNOUN\nPUNCT\nADV\nPUNCT\nDET\nNUM\nPUNCT\nNOUN\nNOUN\nPUNCT\nPUNCT\nADJ\nNOUN\nVERB\nNOUN\nPART\nAUX\nVERB\nADP\nDET\nADJ\nNOUN\nCCONJ\nPRON\nVERB\nDET\nNOUN\nADP\nNOUN\nPART\nAUX\nVERB\nCCONJ\nVERB\nPUNCT\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "f_read = open('./text.txt', 'r')\n",
    "text = f_read.read()\n",
    "f_read.close()\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "W analizowanym tekscie jest  20  zdan\n"
     ]
    }
   ],
   "source": [
    "#liczba zdan\n",
    "#cos dziwnie bo nie da sie znalezc dlugosci generatora (doc.sents)\n",
    "maxlen=1\n",
    "for sent_i, sent in enumerate(doc.sents):\n",
    "    maxlen=maxlen+1\n",
    "print(\"W analizowanym tekscie jest \", maxlen, \" zdan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "W analizowanym tekscie jest 433  slow\n"
     ]
    }
   ],
   "source": [
    "#liczba slow\n",
    "# trzeba odfiltrowac rzeczy co nie sa slowami, typu tokeny zawierajace same cyfry itd.\n",
    "def is_word_token(token):\n",
    "    return token.pos_ not in ['PUNCT', 'SYM', 'NUM', 'SPACE']\n",
    "\n",
    "print(\"W analizowanym tekscie jest\", len(list(filter(is_word_token, doc))), \" slow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "W zdaniach srednio jest  27.0  tokenów\n"
     ]
    }
   ],
   "source": [
    "#srednia liczba tokenow w zdaniu (TOKENOW, nie slow)\n",
    "doc_sent = nlp(text)\n",
    "tokens_in_each_sentence = []\n",
    "for sent_i, sent in enumerate(doc_sent.sents):\n",
    "  tokens_in_each_sentence.append(len(sent))\n",
    "\n",
    "print(\"W zdaniach srednio jest \", sum(tokens_in_each_sentence)/len(tokens_in_each_sentence), \" tokenów\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Liczba rzeczowników  143\nLiczba czasowników  47\nLiczba przymiotników  53\nLiczba przyslowkow  18\n"
     ]
    }
   ],
   "source": [
    "#liczba rzeczownikow, czasownikow, przymiotnikow i przyslowkow\n",
    "#rzeczownik\n",
    "def is_noun(token):\n",
    "    return token.pos_==\"NOUN\"\n",
    "#czasownik\n",
    "def is_verb(token):\n",
    "    return token.pos_==\"VERB\"\n",
    "#przymiotnik\n",
    "def is_adjective(token):\n",
    "    return token.pos_==\"ADJ\"\n",
    "#przyslowek\n",
    "def is_adverb(token):\n",
    "    return token.pos_==\"ADV\"\n",
    "\n",
    "\n",
    "print(\"Liczba rzeczowników \",sum(is_noun(token) for token in doc))\n",
    "print(\"Liczba czasowników \",sum(is_verb(token) for token in doc))\n",
    "print(\"Liczba przymiotników \",sum(is_adjective(token) for token in doc))\n",
    "print(\"Liczba przyslowkow \",sum(is_adverb(token) for token in doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5 najczęściej występujących zlematyzowanych słow to:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('computer', 18),\n",
       " ('device', 13),\n",
       " ('operation', 6),\n",
       " ('machine', 3),\n",
       " ('system', 3)]"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "import collections\n",
    "#5 najczescej wystepujacych rzeczownikow w podsatawowej formie (lematyzacja)\n",
    "noun_tokens = filter(is_noun, doc)\n",
    "noun_lemmas = list(map(lambda token: token.lemma_, noun_tokens))\n",
    "print(\"5 najczęściej występujących zlematyzowanych słow to:\")\n",
    "collections.Counter(noun_lemmas).most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Modern\ncomplete\nindustrial\nsimple\nspecial\ngeneral\npersonal\nmobile\nother\nEarly\nsimple\nmanual\nmechanical\nmodern\nPeripheral\nPeripheral\n"
     ]
    }
   ],
   "source": [
    "#dla dwóch najczęściej występujących rzeczowników wyświetli określające go przymiotniki występujące w tekście\n",
    "for chunk in filter(lambda x: x.root.lemma_ == 'computer' or x.root.lemma_ == 'device', doc.noun_chunks):\n",
    "    for token in chunk:\n",
    "        if is_adjective(token): print(token)"
   ]
  }
 ]
}